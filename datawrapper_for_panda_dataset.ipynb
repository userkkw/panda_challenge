{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "datawrapper_for_panda_dataset.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNxmwvcfxJ94Azps52XhA6E",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/userkkw/panda_challenge/blob/master/datawrapper_for_panda_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1c_3dJ4iMjRB",
        "colab_type": "code",
        "outputId": "e403c6f6-9eee-4777-f3b4-2fd4ae95bd6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBjm1NFHNETS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils, datasets\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from gaussian_blur import GaussianBlur\n",
        "\n",
        "np.random.seed(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bleRJZCIMvjo",
        "colab_type": "code",
        "outputId": "1c98369a-6c04-48d5-ec54-e2f97fa5ea55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#labels for each patch\n",
        "labels_frame = pd.read_csv('/content/drive/My Drive/patch_labels.csv')\n",
        "labels_frame.shape"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(31, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tq1s3rWVX5lJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#create the symbol link to use datasets.ImageFolder command\n",
        "import tempfile\n",
        "src_dir = '/content/drive/My Drive/a51ad4ec379a9209c740025a6d410708' #where the images is located\n",
        "root = tempfile.TemporaryDirectory().name\n",
        "for i in range(len(labels_frame)):\n",
        "  img_name, label = labels_frame['image'][i], str(labels_frame['grade'][i])\n",
        "  os.makedirs(os.path.join(root, label), exist_ok=True)\n",
        "  os.symlink(os.path.join(src_dir, img_name), os.path.join(root, label, img_name))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DTFCFaJqUqw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#dataset_wrapper.py with change of train_dataset to our dataset\n",
        "class SimCLRDataTransform(object):\n",
        "    def __init__(self, transform):\n",
        "        self.transform = transform\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        xi = self.transform(sample)\n",
        "        xj = self.transform(sample)\n",
        "        return xi, xj\n",
        "\n",
        "class DataSetWrapper(object):\n",
        "\n",
        "    def __init__(self, batch_size, num_workers, valid_size, input_shape, s):\n",
        "        self.batch_size = batch_size\n",
        "        self.num_workers = num_workers\n",
        "        self.valid_size = valid_size\n",
        "        self.s = s\n",
        "        self.input_shape = eval(input_shape)\n",
        "\n",
        "    def get_data_loaders(self):\n",
        "        data_augment = self._get_simclr_pipeline_transform()\n",
        "\n",
        "        train_dataset = datasets.ImageFolder(root=root,\n",
        "                                       transform=SimCLRDataTransform(data_augment)) #the only change need to make\n",
        "\n",
        "        train_loader, valid_loader = self.get_train_validation_data_loaders(train_dataset)\n",
        "        return train_loader, valid_loader\n",
        "\n",
        "    def _get_simclr_pipeline_transform(self):\n",
        "        # get a set of data augmentation transformations as described in the SimCLR paper.\n",
        "        color_jitter = transforms.ColorJitter(0.8 * self.s, 0.8 * self.s, 0.8 * self.s, 0.2 * self.s)\n",
        "        data_transforms = transforms.Compose([transforms.RandomResizedCrop(size=self.input_shape[0]),\n",
        "                                              transforms.RandomHorizontalFlip(),\n",
        "                                              transforms.RandomApply([color_jitter], p=0.8),\n",
        "                                              transforms.RandomGrayscale(p=0.2),\n",
        "                                              transforms.ToTensor()])\n",
        "        return data_transforms\n",
        "\n",
        "    def get_train_validation_data_loaders(self, train_dataset):\n",
        "        # obtain training indices that will be used for validation\n",
        "        num_train = len(train_dataset)\n",
        "        indices = list(range(num_train))\n",
        "        np.random.shuffle(indices)\n",
        "\n",
        "        split = int(np.floor(self.valid_size * num_train))\n",
        "        train_idx, valid_idx = indices[split:], indices[:split]\n",
        "\n",
        "        # define samplers for obtaining training and validation batches\n",
        "        train_sampler = SubsetRandomSampler(train_idx)\n",
        "        valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "        train_loader = DataLoader(train_dataset, batch_size=self.batch_size, sampler=train_sampler,\n",
        "                                  num_workers=self.num_workers, drop_last=True, shuffle=False)\n",
        "\n",
        "        valid_loader = DataLoader(train_dataset, batch_size=self.batch_size, sampler=valid_sampler,\n",
        "                                  num_workers=self.num_workers, drop_last=True)\n",
        "        return train_loader, valid_loader\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}